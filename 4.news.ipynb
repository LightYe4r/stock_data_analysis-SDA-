{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3fa1c2-726d-41b6-b7cf-538ba40418bc",
   "metadata": {},
   "source": [
    "##  import Beautifulsoup, request, selenium 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17c795a6-7b89-4236-b718-8661e3f9b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, element\n",
    "import chardet\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cf3c1-0971-43d3-a2a0-541817ccbc95",
   "metadata": {},
   "source": [
    "### 네이버 크롤링 위해 고정 header 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "043cfbdd-3cee-4b7e-aef0-479589c2c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent' : 'Mozilla/5.0F(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "resultList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f4447-4b59-454b-9baa-971af910b243",
   "metadata": {},
   "source": [
    "### Chrome 드라이버 생성 , headless 모드(웹브라우저 실행없이)로 실행, SSSL 인증서 오류 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd61e373-77f3-43ee-9856-94023ee88d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument('--ignore-certificate-errors') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091401e2-913b-4ad1-8f06-4df8c9849321",
   "metadata": {},
   "source": [
    "#   크롤링 함수 정의 \n",
    "### 뉴스 검색 사이트 URL 부분의 검색명에 {회사 이름}를 넣어줌  \n",
    "### 뉴스 제목, 뉴스 날짜, 언론사를 크롤링\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aba1986a-1842-4b23-89d5-dd55a14896c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_crawl(start_date, end_date,company):\n",
    "\n",
    "    start_date_nospace = start_date.replace('.', '')\n",
    "    end_date_nopspace = end_date.replace('.', '')\n",
    "    current_page = 1\n",
    "    searching = True\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    search_urlback = f\"{start_date}&de={end_date}&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{start_date_nospace}to{end_date_nopspace},a:all&start=1\"\n",
    "    search_urlfront = f\"https://search.naver.com/search.naver?where=news&query={company}&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds=\"\n",
    "    search_url = search_urlfront + search_urlback\n",
    "    print(search_url)\n",
    "\n",
    "    while searching:\n",
    "\n",
    "        driver.get(search_url)\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        # 페이지의 끝까지 스크롤 다운\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # 로딩 대기 시간 설정 (필요에 따라 조절)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # 새로운 컨텐츠가 로드될 때까지 대기\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "        print(last_height)\n",
    "        \n",
    "        naver_html_src = driver.page_source\n",
    "        naver_soup = BeautifulSoup(naver_html_src, 'html.parser')\n",
    "        pages = naver_soup.find('div', {'class' : 'sc_page_inner'})\n",
    "\n",
    "        # 뉴스 리스트 \n",
    "        news_list = naver_soup.select('li[class=\"bx\"]')\n",
    "\n",
    "        for news in news_list:\n",
    "            time.sleep(2)\n",
    "            # 뉴스 제목\n",
    "            try :\n",
    "                news_title = news.find('a', attrs={'class':'news_tit'}).get('title')\n",
    "            except AttributeError:\n",
    "                news_title = \"No title available\"\n",
    "\n",
    "            # 뉴스 원본 링크 \n",
    "            news_url = news.find('a', attrs={'class':'news_tit'}).get('href')\n",
    "\n",
    "            # 뉴스 날짜 \n",
    "            find_date = news.find_all('span', attrs={'class':'info'})\n",
    "            if len(find_date) > 1:\n",
    "                news_date = find_date[1].text\n",
    "            else: \n",
    "                news_date = find_date[0].text\n",
    "\n",
    "            # 뉴스 발행사 \n",
    "            try:\n",
    "                news_publisher = news.find('a', attrs={'class':'info press'}).text\n",
    "            except:\n",
    "                try:\n",
    "                    news_publisher = news.find('a', attrs={'class':'info'}).text\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "            # 뉴스 내용\n",
    "            news_summary = news.find('a', attrs={'class':'api_txt_lines dsc_txt_wrap'}).text\n",
    "            news_summary = news_summary[0:50]\n",
    "\n",
    "            news_text = []\n",
    "\n",
    "            # if \"일\" in news_date:\n",
    "            #     news_date = datetime.today() - timedelta(days=int(news_date[0]))\n",
    "            #     news_date = news_date.strftime(\"%Y-%m-%d\")\n",
    "            # print(news_date)\n",
    "            try:\n",
    "                news_response = requests.get(news_url, headers=headers)\n",
    "            except :\n",
    "                continue\n",
    "\n",
    "            if news_summary in str(news_response.text):\n",
    "                news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            else:\n",
    "                news_soup = BeautifulSoup(news_response.content.decode('euc-kr', 'replace'), 'html.parser')\n",
    "            news_content = str(news_soup.text).split('\\n')\n",
    "            news_index = [i for i, s in enumerate(news_content) if news_summary in s]\n",
    "            \n",
    "            if news_index:\n",
    "                news_index = news_index[0]\n",
    "                punctuations = ['.', ',', '?', '!']\n",
    "                count = 0\n",
    "                if news_index != 0:\n",
    "                    for i in reversed(range(0, news_index)):\n",
    "                        if any(text in news_content[i] for text in punctuations): \n",
    "                                news_text.append(news_content[i])\n",
    "                                count = 0\n",
    "                        else:\n",
    "                            count +=1\n",
    "                        if count > 3:\n",
    "                            break\n",
    "\n",
    "                if news_text:\n",
    "                    news_text.reverse()\n",
    "                count = 0\n",
    "                news_text.append(news_content[news_index])\n",
    "                for i in range(news_index, len(news_content)):\n",
    "                    \n",
    "                    if any(text in news_content[i] for text in punctuations): \n",
    "                        news_text.append(news_content[i])\n",
    "                        count = 0\n",
    "\n",
    "                    else:\n",
    "                        count +=1\n",
    "                    if count > 3:\n",
    "                        break\n",
    "                \n",
    "                news_text = ' '.join(news_text)\n",
    "\n",
    "            resultList.append([news_date, news_title, news_text, news_publisher])\n",
    "            df = pd.DataFrame(resultList)\n",
    "\n",
    "            # updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "            df.to_csv(f'news_{company}3.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    return news_title, news_date\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31a20e-eb82-469e-bcf4-fa87833eb5bd",
   "metadata": {},
   "source": [
    "# 함수 실행\n",
    "### 시작날짜, 끝날짜, 회사이름\n",
    "### 중간에 언론사 or 네이버 서버에 의해 네트워크 중지되었다는 오류 발생할 수도 있음-=> .py로 실행\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128aa299-3a1f-453a-b4a4-acd2743c5daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&query=유진테크&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds=20240101&de=20240302&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20240101to20240302,a:all&start=1\n",
      "4324\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "start_crawl('20240101','20240302','유진테크')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a264bc9-8d57-42b9-ad09-0f560199960c",
   "metadata": {},
   "source": [
    "## 크롤링 파일 합치기\n",
    "## 파일들의 리스트를 얻음\n",
    "## 각 파일을 DataFrame으로 읽은 후 리스트에 저장\n",
    "## 병합된 DataFrame을 CSV 파일로 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "add017f8-bc5f-4883-9641-694b0f601b8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m file_list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_list]\n\u001b[1;32m----> 9\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewslast.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "file_list = glob.glob(f'news_*.csv')\n",
    "\n",
    "dfs = [pd.read_csv(file) for file in file_list]\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv(f'newslast.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9297b-fe48-43b4-968e-8802c5a983ef",
   "metadata": {},
   "source": [
    "# WordCloud "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09f324-610d-4a0e-8d00-adabd77535fc",
   "metadata": {},
   "source": [
    "## konlpy를 위한 jdk 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b27d4-f4b5-4f2e-91b2-00399105060b",
   "metadata": {},
   "source": [
    "https://www.oracle.com/kr/java/technologies/javase/javase8u211-later-archive-downloads.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba6237-3569-45e5-a08b-4d5478d3b3a1",
   "metadata": {},
   "source": [
    "윈도우 > 시스템 환경 변수 편집 > 환경변수 > 시스템 변수 > 새로 만들기\n",
    "\n",
    "변수 : JAVA_HOME         \n",
    "값(jdk 설치 경로) : C:\\Program Files\\Java\\jdk-1.8\n",
    "\n",
    "윈도우 > 시스템 환경 변수 편집 > 환경변수 > 시스템 변수 > Path > 편집 > 새로 만들기             \n",
    "                          \n",
    "%JAVA_HOME%\\bin;\n",
    "\n",
    "윈도우 cmd에서 $java -version      \n",
    "으로 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a07c4a4a-9c73-4759-82b5-6ec678ce5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0ed859a-5443-49e1-b0d0-1aebad605807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wordcloud) (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: kss in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.5.4)\n",
      "Requirement already satisfied: emoji==1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kss) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kss) (2023.12.25)\n",
      "Requirement already satisfied: pecab in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kss) (1.0.8)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kss) (3.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pecab->kss) (1.26.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pecab->kss) (15.0.1)\n",
      "Requirement already satisfied: pytest in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pecab->kss) (8.1.1)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest->pecab->kss) (2.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest->pecab->kss) (24.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest->pecab->kss) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytest->pecab->kss) (0.4.6)\n",
      "Requirement already satisfied: konlpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from konlpy) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (24.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install kss\n",
    "!pip install konlpy\n",
    "!pip install scikit-learn\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21047d06-d023-4744-a055-86a1829ee4aa",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d1177-9ea2-49a8-91ba-25a65ac905c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import base64\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import certifi\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "font_fname = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_family = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "plt.rcParams['font.family'] = font_family\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import kss\n",
    "from konlpy.tag import Okt, Kkma\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992b810-8380-41a9-9246-1211951ed374",
   "metadata": {},
   "source": [
    "# 필요한 파일 준비\n",
    "\n",
    "## stopwords-ko.txt 파일과 같은 경로에 다운로드\n",
    "\n",
    "## 파일과 같은 경로에 wordcloud_mask.png 이름의 wordcloud 생성을 위한 mask 이미지 저장\n",
    "\n",
    "# Text에서 stopwords 제거 및 문장 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b0946-dd57-4927-b7dc-b31729d3853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.okt = Okt()\n",
    "        self.kkma = Kkma()\n",
    "        self.stopwords = []\n",
    "        stopwords_path = './stopwords-ko.txt'\n",
    "        with open(stopwords_path, 'r', encoding='UTF8') as file:\n",
    "            for line in file:\n",
    "                self.stopwords.append(line.strip())\n",
    "\n",
    "    def text2sentences(self, text):\n",
    "        sentences = kss.split_sentences(text)\n",
    "        return sentences\n",
    "        \n",
    "    def sentences2nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            for word in self.okt.nouns(str(sentence)):\n",
    "                if word in self.stopwords or len(word) <= 1:\n",
    "                    continue    \n",
    "                if word in nouns:\n",
    "                    continue\n",
    "                nouns.append(word)\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59de07e-c536-4f1a-a81a-7a60adaaa6aa",
   "metadata": {},
   "source": [
    "#  단어 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fd66a-7170-4415-a2aa-04d86f03b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.text_analyze = TextAnalyzer()\n",
    "        self.countvec = CountVectorizer()\n",
    "        self.okt = Okt()\n",
    "        self.stoptags = ['Determiner', 'Adverb', 'Conjunction', 'Exclamation', 'Josa']\n",
    "        self.counttags = ['Noun', 'Verb', 'Adjective']\n",
    "        self.essential_josa = ['은', '는', '이', '가', '을', '를']\n",
    "        \n",
    "    def build_word_graph(self, sentence):\n",
    "        countvec_mat = normalize(self.countvec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.countvec.vocabulary_\n",
    "        return np.dot(countvec_mat.T, countvec_mat), {vocab[word]: word for word in vocab}\n",
    "    \n",
    "    def get_ranks(self, graph, d=0.85):\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0    \n",
    "            link_sum = np.sum(A[:, id])\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B)\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "    # 키워드\n",
    "    def text2keywords(self, text, word_num=15):        \n",
    "        sentences = self.text_analyze.text2sentences(text)\n",
    "        nouns = self.text_analyze.sentences2nouns(sentences)\n",
    "        word_graph, idx2word = self.build_word_graph(nouns)\n",
    "        word_rank_idx = self.get_ranks(word_graph)\n",
    "        sorted_word_rank_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index = []\n",
    "        \n",
    "        for idx in sorted_word_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        for idx in index:\n",
    "            keywords.append(idx2word[idx])\n",
    "        return keywords\n",
    "    \n",
    "    # 품사 태깅\n",
    "    def text2postag(self, text):\n",
    "        postag = self.okt.pos(text)\n",
    "        return postag\n",
    "    \n",
    "    # 빈도수 높은 단어\n",
    "    def text2countwords(self, text):\n",
    "        postag = self.text2postag(text)\n",
    "        countwords_postag = []\n",
    "        \n",
    "        for i in range(len(postag)):\n",
    "            if postag[i][1] in self.counttags:\n",
    "                countwords_postag.append(postag[i][0])\n",
    "        return countwords_postag\n",
    "    \n",
    "    # visualization\n",
    "    def words2wordscount(self, words, counttype):\n",
    "        if counttype == 'individual':\n",
    "            whole = []\n",
    "            for i in range(len(words)):\n",
    "                for j in range(0, len(words)-i, 1):\n",
    "                    whole.append(words[i])\n",
    "            words = whole\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 모든 경우에서 count를 통해 dict 생성\n",
    "        count = Counter(words)\n",
    "        wordscount = dict(count.most_common())\n",
    "        # countwoords 선정시 단어 길이, 빈도수 1인 단어 제거\n",
    "        if counttype == 'count':\n",
    "            dict_key = list(wordscount.keys())\n",
    "            for i in dict_key:\n",
    "                if wordscount[i] == 1:\n",
    "                    del(wordscount[i])\n",
    "        else:\n",
    "            pass\n",
    "        return wordscount\n",
    "    \n",
    "\n",
    "\n",
    "    # 워드클라우드 시각화\n",
    "    def visualize_wordcloud(self, text, wordtype):\n",
    "        #wordcloud_path = os.getcwd()+'\\\\server\\\\routers\\\\text_analysis'\n",
    "        wordcloud_path = '.'\n",
    "        img_save_path = '.'\n",
    "        if wordtype == 'keywords':\n",
    "            mask = np.array(Image.open(wordcloud_path+'/기영이.jpg'))\n",
    "            words = self.text2keywords(text)\n",
    "            words = self.words2wordscount(words, 'individual')\n",
    "        \n",
    "        elif wordtype == 'countwords':\n",
    "            mask = np.array(Image.open(wordcloud_path+'/기영이.jpg'))\n",
    "            words = self.text2countwords(text)\n",
    "            words = self.words2wordscount(words, 'count')\n",
    "        \n",
    "        image_colors = ImageColorGenerator(mask)\n",
    "        wordcloud = WordCloud(font_path=font_fname, background_color='white',\n",
    "                              mask=mask, width=mask.shape[1], height=mask.shape[0], prefer_horizontal=0.99999)\n",
    "        cloud = wordcloud.generate_from_frequencies(words)\n",
    "        \n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(cloud.recolor(color_func=image_colors), interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(img_save_path+f'/text_wordcloud_{wordtype}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277bd51-6069-4c30-945d-3c366b49fcf9",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269dc0a6-eee8-47a6-b7ac-04bf1a49a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "company = \"피에스케이\"\n",
    "news = pd.read_csv(f'news{company}last.csv', encoding='utf-8-sig')\n",
    "news_title = news[\"1\"].str.strip()\n",
    "news_title_value = ' '.join(news_title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535a64f-f479-414d-9024-547d76e9c884",
   "metadata": {},
   "source": [
    "# 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5422689c-80fc-4052-9667-389424176338",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_title_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     stttext \u001b[38;5;241m=\u001b[39m \u001b[43mnews_title_value\u001b[49m\n\u001b[0;32m      3\u001b[0m     img_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m     TA \u001b[38;5;241m=\u001b[39m TextAnalyzer()     \u001b[38;5;66;03m# 텍스트 분석 클래스\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_title_value' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    stttext = news_title_value\n",
    "    img_save_path = '-'\n",
    "    \n",
    "    TA = TextAnalyzer()     # 텍스트 분석 클래스\n",
    "    WA = WordAnalyzer()     # 단어 분석 클래스\n",
    "\n",
    "    keywords      = WA.text2keywords(stttext)                   \n",
    "    top3_keywords = list(WA.words2wordscount(keywords, 'individual'))[:3]         # 키워드 상위 3개\n",
    "    countwords      = WA.text2countwords(stttext)                 \n",
    "    top3_countwords = list(WA.words2wordscount(countwords, 'countwords'))[:3]     # 빈도수 높은 단어 상위 3개\n",
    "    WA.visualize_wordcloud(stttext, 'keywords')       # 키워드 워드클라우드\n",
    "    WA.visualize_wordcloud(stttext, 'countwords')     # 빈도수 높은 단어 워드클라우드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff26c7-63a6-4d61-addb-0d1bf6b4934d",
   "metadata": {},
   "source": [
    "# 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b3d6c-add0-4f24-887c-11772ec383e7",
   "metadata": {},
   "source": [
    "## pytorch, tensorflow 설치하기 싫으니 colab으로 실행 해준다\n",
    "### 허깅페이스 api 사용 => pipeline(\"task\", \"모델명\")\n",
    "### 예시 sentiment_classification(\"today is payday!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be42af-3e73-4ca5-8cc0-a5ba863dbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_classification = pipeline(\"text-classification\", \"snunlp/KR-FinBert-SC\")\n",
    "\n",
    "import pandas as pd \n",
    "# sentiment_classification(\"today is payday!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245fd09-fc46-4cbf-846a-237bc047d9e6",
   "metadata": {},
   "source": [
    "## 해당하는 회사 이름 csv 불러오기 (경로 확인하기, 파일을 꼭 colab에 업로드해 줘야함!!), 열 이름 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73fc6630-5328-4a37-a0ac-db8d57aad0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "company = \"DB하이텍\"\n",
    "news = pd.read_csv(f'/content/news{company}last.csv', encoding='utf-8-sig')\n",
    "news.rename(columns={news.columns[0] : '날짜', \n",
    "                          news.columns[1] : '제목', \n",
    "                          news.columns[2] : '내용', \n",
    "                          news.columns[3] : '언론사'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817d2d7-d532-44b5-904f-62c716e5dc7b",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e069f19-afe9-465b-a166-139a36db8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = news.dropna()\n",
    "# news = news.drop(1)\n",
    "# 데이터 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 괄호와 괄호 안 문자 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    # '...' 제거\n",
    "    text = text.replace('...', ' ')\n",
    "    return text\n",
    "news_data = news[\"1\"].apply(preprocess_text)\n",
    "print(news_data)\n",
    "\n",
    "# ## 뉴스 본론 데이터 전처리\n",
    "# news_contents_raw = news[news['2'] != '[]']\n",
    "# news_contents = news_contents_raw[\"2\"].str.strip()\n",
    "# # news_contents_value = ' '.join(news_contents.values)\n",
    "# # print(news_contents_value)\n",
    "# print(news_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ead59-a344-429f-b30a-50a0a891eea7",
   "metadata": {},
   "source": [
    "## 뉴스 데이터 중에서 해당 회사 이름이 포함된 내용만 가져오기\n",
    "### 주의 사항 : huggingface에서 (The size of tensor a (1624) must match the size of tensor b (512) at non-singleton dimension 1) 글자 수 제한 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "535e7b06-3efd-4b72-83b9-d9408ce51f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3855880390.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    if news_data True:\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 제목 열 가져오기\n",
    "news_title = news[\"1\"].str.strip()\n",
    "# print(news_title)\n",
    "news_title_data = news_title.str.contains(company)\n",
    "\n",
    "# 뉴스 제목에서 해당 회사 이름 포함된 내용만 \n",
    "for index, row in news_title.iteritems():\n",
    "    if news_title_data[index]:\n",
    "        news_title_value = ''.join(row)\n",
    "        print(news_title_value)\n",
    "\n",
    "\n",
    "# # 뉴스 내용 열 가져오기\n",
    "# news_contents = news[\"2\"].str.strip()\n",
    "# news_contents_data = news_contents.str.contains(company) \n",
    "\n",
    "# # 뉴스 내용에서 해당 회사 이름 포함된 내용만\n",
    "# for index, row in news_contents.iteritems():\n",
    "#   if news_contents_data[index]:\n",
    "#      news_contents_value = ''.join(row)\n",
    "#      print(news_contents_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f6332a-5728-4b01-8801-94192bff5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb29ae-593b-42da-9436-4681c6a4afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classification(news_title_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
